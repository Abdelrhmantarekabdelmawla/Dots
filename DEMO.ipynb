{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import Runnable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyC-jh6w2PyKRX6MOTnWG6WJFWpNecDiDbY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IT SHOP\\AppData\\Local\\Temp\\ipykernel_5028\\4037422431.py:48: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Define a function to generate text using Gemini\n",
    "def generate_text(prompt):\n",
    "    response = model.generate_content(prompt.text)\n",
    "    if response and response.text:\n",
    "        return response.text\n",
    "    return \"No response content available.\"\n",
    "\n",
    "# Create a custom LLM class implementing Runnable interface\n",
    "class CustomLLM(Runnable):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def invoke(self, input_str: str, config=None, **kwargs) -> str:\n",
    "        return generate_text(input_str)\n",
    "\n",
    "    def __call__(self, input_str: str, config=None, **kwargs) -> str:\n",
    "        return self.invoke(input_str, config=config, **kwargs)\n",
    "\n",
    "def add_to_memory(user_input, response):\n",
    "    memory.save_context(user_input, response)\n",
    "    \n",
    "# Create an instance of your custom LLM\n",
    "custom_llm = CustomLLM(model)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    انت بالنسبالي خبير في المجال التكنولوجي والتقني، عندك خبرة في مجال السوفت ويير لأكثر من 25 سنة، ومطلع على كل التطورات في المجال، وكمان عندك خبرة في مجال التحليل الخاص بالشخصيات، انت بالنسبة ليا هتكون حد موثوق جداً. عايزك تقولي أسئلة بشكل اختيار من متعدد، وعايز الاسئلة تكون عبارة عن 10 أسئلة، الاسئلة دي لما هجاوبها انت هتحدد ليا مجال تقني من المجالات المتاحة زي مثلاً تطوير المواقع أو تطبيقات الموبايل أو أي مجال من المجالات التقنية بشرط ان عدد المجالات اللي ممكن تدور فيها لازم متكونش أزيد من 10 مجالات وموزعين بشكل كويس، وكمان الاسئلة اللي هتسئلها تكون بتقيس مقدار الفهم يعني هنبدأ من أول هل انا حابب المجال التقني ولو أيوا فإيه السبب وبد كدا سؤال أعمق شويةن المهم بعد ما أجاوبك على 10 أسئلة تكون قادر ترشحلي مجال ودا بيقيس أصلاً ايه المجال المناسب ليا. هتبدأ الأسئلة بشكل متتابع بحيث لما أجاوبك على سؤال تسألني السؤال اللي بعده. وبعد ما اخلص الـ 10 أسئلة قولي تحليل بسيط وواضح وعلى أساسه ترشحلي المجال.  خلي الأسئلة تكون بالإنجليزي، والتحليل برده في النهاية بالإنجليزي وترشيح المجال بالإنجليزي ومتشرحليش انت هتعمل ايه اول ما ابعتلك أي رسالة ابدا ابعتلي اول سؤال علطول ووبعدها استنى مني الرد وابعت التاني وهكذا لحد بعد اخر سؤال ابعتلي التراك المقترح وعايز بين السؤال وكل إجابة علامة \"_\" عشان اعرف افصل بينهم\n",
    "\n",
    "    Here is the conversation so far:\n",
    "    {history}\n",
    "    User: {input}\n",
    "    Assistant:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Set up memory to keep track of the conversation\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# هنا يسطا عشان تضيف الرساله اللي في الاول\n",
    "add_to_memory({\"input\":\"\"}, {\"response\": \"Hello, how can I help you today?\"})\n",
    "\n",
    "\n",
    "# Create a conversation chain with the prompt and memory\n",
    "conversation = ConversationChain(\n",
    "    llm=custom_llm,\n",
    "    prompt=prompt_template,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# Function to have a conversation\n",
    "def converse(user_input):\n",
    "    # Prepare inputs for the conversation chain\n",
    "    inputs = {\n",
    "        # \"roles\": roles,\n",
    "        \"input\": user_input,\n",
    "        \"history\": memory.chat_memory\n",
    "    }\n",
    "    response = conversation(inputs)[\"response\"]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, based on your answers, here's my analysis and recommended field:\n",
      "\n",
      "**Analysis:**\n",
      "\n",
      "You consistently demonstrated a preference for structured, methodical approaches (questions 2, 4, 10).  While you enjoy problem-solving (question 1), you lean towards well-defined problems (1b). Your interest in implementation and coding (question 3), combined with your comfort level (though not complete mastery) with mathematical concepts (question 5), points towards a role where practical application is emphasized, but theoretical understanding is also valued.  You prefer a contributor role (question 6) and are willing to learn new technologies (question 8), but your preference leans towards functionality over visual design (questions 7 and 9).\n",
      "\n",
      "**Recommendation:**\n",
      "\n",
      "Based on your responses, I recommend exploring **Data Science** or a related field such as **Machine Learning Engineering**.  Your preference for structured approaches and comfort with (though not necessarily love of) mathematical concepts align well with these fields. The focus on problem-solving and implementation also fits the data science workflow.  Although you expressed a preference for the front-end in question 9, the analytical and problem-solving skills exhibited make data science a more suitable long-term fit given your answers overall.  The methodical nature of many data science tasks fits your preference for structured environments.  Furthermore, many data science roles offer a good balance between individual contributions and collaborative team efforts, accommodating your preference for a contributor role.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Okay, based on your answers, here's my analysis and recommended field:\\n\\n**Analysis:**\\n\\nYou consistently demonstrated a preference for structured, methodical approaches (questions 2, 4, 10).  While you enjoy problem-solving (question 1), you lean towards well-defined problems (1b). Your interest in implementation and coding (question 3), combined with your comfort level (though not complete mastery) with mathematical concepts (question 5), points towards a role where practical application is emphasized, but theoretical understanding is also valued.  You prefer a contributor role (question 6) and are willing to learn new technologies (question 8), but your preference leans towards functionality over visual design (questions 7 and 9).\\n\\n**Recommendation:**\\n\\nBased on your responses, I recommend exploring **Data Science** or a related field such as **Machine Learning Engineering**.  Your preference for structured approaches and comfort with (though not necessarily love of) mathematical concepts align well with these fields. The focus on problem-solving and implementation also fits the data science workflow.  Although you expressed a preference for the front-end in question 9, the analytical and problem-solving skills exhibited make data science a more suitable long-term fit given your answers overall.  The methodical nature of many data science tasks fits your preference for structured environments.  Furthermore, many data science roles offer a good balance between individual contributions and collaborative team efforts, accommodating your preference for a contributor role.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Okay, based on your answers, here's my analysis and recommended field:\\n\\n**Analysis:**\\n\\nYou consistently demonstrated a preference for structured, methodical approaches (questions 2, 4, 10).  While you enjoy problem-solving (question 1), you lean towards well-defined problems (1b). Your interest in implementation and coding (question 3), combined with your comfort level (though not complete mastery) with mathematical concepts (question 5), points towards a role where practical application is emphasized, but theoretical understanding is also valued.  You prefer a contributor role (question 6) and are willing to learn new technologies (question 8), but your preference leans towards functionality over visual design (questions 7 and 9).\\n\\n**Recommendation:**\\n\\nBased on your responses, I recommend exploring **Data Science** or a related field such as **Machine Learning Engineering**.  Your preference for structured approaches and comfort with (though not necessarily love of) mathematical concepts align well with these fields. The focus on problem-solving and implementation also fits the data science workflow.  Although you expressed a preference for the front-end in question 9, the analytical and problem-solving skills exhibited make data science a more suitable long-term fit given your answers overall.  The methodical nature of many data science tasks fits your preference for structured environments.  Furthermore, many data science roles offer a good balance between individual contributions and collaborative team efforts, accommodating your preference for a contributor role.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\", line 203, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 791, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 497, in _make_request\n",
      "    conn.request(\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\", line 395, in request\n",
      "    self.endheaders()\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1274, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1034, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 974, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\", line 243, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\", line 218, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002617E9C8190>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 845, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002617E9C8190>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 1286, in run\n",
      "    self.function(*self.args, **self.kwargs)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\flask_ngrok.py\", line 70, in start_ngrok\n",
      "    ngrok_address = _run_ngrok()\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\flask_ngrok.py\", line 35, in _run_ngrok\n",
      "    tunnel_url = requests.get(localhost_url).text  # Get the tunnel information\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"c:\\Users\\IT SHOP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002617E9C8190>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "127.0.0.1 - - [01/Dec/2024 15:09:23] \"GET /api/converse HTTP/1.1\" 405 -\n",
      "127.0.0.1 - - [01/Dec/2024 15:09:55] \"GET /api/converse HTTP/1.1\" 405 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Automatically creates an ngrok tunnel\n",
    "\n",
    "# Reset memory function\n",
    "def reset_memory():\n",
    "    global memory\n",
    "    memory = ConversationBufferMemory()\n",
    "    add_to_memory({\"input\": \"\"}, {\"response\": \"Hello, how can I help you today?\"})\n",
    "\n",
    "\n",
    "# Endpoint to converse with the model\n",
    "@app.route('/api/converse', methods=['POST'])\n",
    "def converse_endpoint():\n",
    "    try:\n",
    "        # Get user input from the request\n",
    "        data = request.get_json()\n",
    "        user_input = data.get(\"input\", \"\")\n",
    "        \n",
    "        if not user_input:\n",
    "            return jsonify({\"error\": \"Input is required\"}), 400\n",
    "        \n",
    "        # Call the converse function\n",
    "        response = converse(user_input)\n",
    "        \n",
    "        # Return the response as JSON\n",
    "        return jsonify({\"response\": response}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "\n",
    "# Endpoint to reset the conversation memory\n",
    "@app.route('/api/reset_memory', methods=['POST'])\n",
    "def reset_memory_endpoint():\n",
    "    try:\n",
    "        # Reset the conversation memory\n",
    "        reset_memory()\n",
    "        return jsonify({\"message\": \"Conversation memory has been reset.\"}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == '__main__':\n",
    "    # Initialize memory at the start\n",
    "    reset_memory()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
